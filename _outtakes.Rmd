---
title: "Untitled"
author: "Ingrid Zoll"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r count_tables2, results="asis", echo=FALSE}
kable(park_counts,
  col.names = c("Species code", "Scientific name", "Collected in 2019", "Total in inventory"),
  caption = "Canary species frequency in the Portland Park Trees Database",
  caption.short = "Species counts in Park Trees Database",
  longtable = TRUE,
  booktabs = TRUE
)

kable(street_counts,
  col.names = c("Species code", "Scientific name", "Collected in 2016", "Total in inventory"),
  caption = "Canary species frequency in the Portland Street Trees Database",
  caption.short = "Species counts in Street Trees Database",
  longtable = TRUE,
  booktabs = TRUE
)
```


\begin{equation}
  \mathrm{NDVI} = \frac{(NIR - Red)}{(NIR + Red)}
\end{equation}


\begin{equation}
  \mathrm{C_6H_{12}O_6  + 6O_2} \longrightarrow \mathrm{6CO_2 + 6H_2O}
  (\#eq:reaction)
\end{equation}



```{r count_tables2, results="asis", echo=FALSE}
kable(park_counts,
  col.names = c("Species code", "Scientific name", "Collected in 2019", "Total in inventory"),
  caption = "TEST CAPTION",
  caption.short = "Species counts in Park Trees Database",
  longtable = TRUE,
  booktabs = TRUE
)
```


```{r test_chunk, echo=FALSE}
kable(park_counts,
      caption = "Test", 
      caption.short = "test2",
      longtable = TRUE,
      booktabs = TRUE)
```


# Graphics, References, and Labels {#ref-labels}


## Figures

If your thesis has a lot of figures, _R Markdown_ might behave better for you than that other word processor.  One perk is that it will automatically number the figures accordingly in each chapter.    You'll also be able to create a label for each figure, add a caption, and then reference the figure in a way similar to what we saw with tables earlier.  If you label your figures, you can move the figures around and _R Markdown_ will automatically adjust the numbering for you.  No need for you to remember!  So that you don't have to get too far into LaTeX to do this, a couple **R** functions have been created for you to assist.  You'll see their use below.

<!--
One thing that may be annoying is the way _R Markdown_ handles "floats" like tables and figures (it's really \LaTeX's fault). \LaTeX\ will try to find the best place to put your object based on the text around it and until you're really, truly done writing you should just leave it where it lies. There are some optional arguments specified in the options parameter of the `label` function.  If you need to shift your figure around, it might be good to look here on tweaking the options argument:  <https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions>

If you need a graphic or tabular material to be part of the text, you can just put it inline. If you need it to appear in the list of figures or tables, it should be placed in a code chunk.
-->


<!-- Note the use of `out.width` as a chunk option here. The resulting
image is 20% of what the linewidth is in LaTeX. You can also center
the image using `fig.align="center"` as shown.-->

\clearpage

<!-- clearpage ends the page, and also dumps out all floats.
  Floats are things like tables and figures. -->
  

Next, we will explore the use of the `out.extra` chunk option, which can be used to shrink or expand an image loaded from a file by specifying `"scale= "`. Here we use the mathematical graph stored in the "subdivision.pdf" file.

```{r subd, results="asis", echo=FALSE, fig.cap="Subdiv. graph", out.extra="scale=0.75"}
include_graphics("figure/subdivision.pdf")
```

Here is a reference to this image: Figure \@ref(fig:subd).  Note that `echo=FALSE` is specified so that the **R** code is hidden in the document.

**More Figure Stuff**

Lastly, we will explore how to rotate and enlarge figures using the `out.extra` chunk option.  (Currently this only works in the PDF version of the book.)

```{r subd2, results="asis", echo=FALSE, out.extra="angle=180, scale=1.1", fig.cap="A Larger Figure, Flipped Upside Down"}
include_graphics("figure/subdivision.pdf")
```





```{r}
## resampling LiDAR raster to 3m
EML_CHM_RESAMP <- aggregate(EML_CHM, fact=3)


## Define a function that will allow for dynamic window size.  
## Essentially, this function should take a CHM cell value (i.e.: the height of the canopy above ground at that location) 
#### returns the radius of the search window. 

# function is based on Brieger et al. 2019 (https://doi.org/10.3390/rs11121447) for tall trees (see table 2)
lin_brieger <- function(x){y <- x * 0.02 + 0.4
y[x < 30] <- 1
y[x > 125] <- 3
return(y)}


# calculate treetop locations based on equation above
EML_ttops <- vwf(CHM = EML_CHM_RESAMP, winFun = lin_brieger, minHeight = 20)

# export file
writeOGR(EML_ttops, "~/Desktop/thesis stuff/thesis qgis files/LiDAR_2", "2019_ttops", driver = "ESRI Shapefile")


```

knitr::write_bib(c(.packages(), "ForestTools"), "packages.bib")

Missing a connection between the canopy delineation techniques and the predictive model (you did both, whereas the abstract reads as if you simply tested three techniques on a model you built).
Missing a take-home sentence about what you found (what were your results?)
Could use a last sentence about the potential impact of this type of work for assessing urban tree health.
Again, maybe drop the open source stuff (you can mention this in the intro as part of why you chose the datasets you did, but I wouldn't say this is a "major finding"). Agree with Anna, focus on adding your results and take home message(s).


This model is what Agresti (2002) calls a cumulative link model. The basic interpretation is as a coarsened version of a latent variable $Y_i$ which has a logistic or normal or extreme-value or Cauchy distribution with scale parameter one and a linear model for the mean. The ordered factor which is observed is which bin $Y_i$ falls into with breakpoints $$\zeta_0 = -\infty < \zeta_1 < \cdots < \zeta_K = \infty$$ This leads to the model $$\mbox{logit} P(Y \le k | x) = \zeta_k - \eta$$ with logit replaced by probit for a normal latent variable, and $\eta$ being the linear predictor, a linear function of the explanatory variables (with no intercept). Note that it is quite common for other software to use the opposite sign for $\eta$ (and hence the coefficients beta).

In the logistic case, the left-hand side of the last display is the log odds of category $k$ or less, and since these are log odds which differ only by a constant for different $k$, the odds are proportional. Hence the term proportional odds logistic regression.

The log-log and complementary log-log links are the increasing functions $F^{-1}(p) = -log(-log(p))$ and $F^{-1}(p) = log(-log(1-p))$; some call the first the 'negative log-log' link. These correspond to a latent variable with the extreme-value distribution for the maximum and minimum respectively.

A proportional hazards model for grouped survival times can be obtained by using the complementary log-log link with grouping ordered by increasing times.


<!--# I second all of Anna's suggestions. I'll add that each of the subsections of your discussion should link directly to a key result or take home message. What is it that you want the reader to learn from all your analyses? Another way to think of Anna's "putting your work in context" is how did your results compare to (1) what you hypothesized and (2) what others doing similar work have found. Did you find anything surprising, unique, interesting given this context? I'll also say that the open source stuff here could take on a full subsection like Anna suggests, which would require you to review alternative approaches and put yours into that context. Alternatively, you could view this more as a methodological choice and not a "finding" that you need to discuss. That would relegate it to the more brief mention in intro and methods. I think this is justified and maybe even necessary so you can fully discuss your actual findings. But, if you feel like you really want the reader to understand the importance of an open source approach, then take Anna's suggestions instead and provide that discussion of the literature. -->

 
Urban trees provide numerous benefits, ranging from aesthetic and environmental to psychological and economical. Tree health is a critical part of urban ecosystem function, and is closely tied to the benefits or lack thereof that urban trees can provide.
With over one million trees in Portland's urban forest, conducting field health assessments to understand the dynamics of tree health in Portland is a nearly impossible goal.
Due to the size of urban forests and the time consuming practice of field health assessments, recent research into tree health has turned to satellite imagery as a tool for evaluating tree health. 
This thesis looks to examine the health ratings of four key tree species in Portland (Acer macrophyllum, Acer platanoides, Pseudotsuga menziesii, and Thuja plicata) from field data collected in the summer of 2021, and the relationship between health rating and NDVI (Normalized Difference Vegetation Index), which is an index of "greenness" commonly used in remote sensing of vegetation. 
This thesis tests three different tree canopy delineation and pixel selection techniques for obtaining NDVI to determine which is the most effective for obtaining tree health information and predicting health rating from NDVI.
I used an ordinal logistic regression model to predict health ratings of poor, fair or good. 
These predictions were based on 1) NDVI, 2) NDVI and tree functional type, or 3) NDVI and tree species. 
The impact of including either tree functional type or tree species in the predictive health model was examined. 
The most effective predictive model differentiated the predictions by species, and used the LiDAR data. 
This final model was most effective for the two maple species (ACMA and ACPL), categorizing trees in all three health categories for ACMA, and two health categories for ACPL. 
The model was ineffective for predicting the health ratings for conifer species.


```{r, warning = F, include = F, eval = F}
#DO YOU NEED TO TAKE THIS OUT???
cnh_point %>%
  bind_cols(as_tibble(round(predict(point_species, cnh_point, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")), method = "point") 
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point()+
  geom_line(aes(group = tree_id_nu), alpha = .5)+
  facet_wrap(~tree_type)

cnh_radius %>%
  bind_cols(as_tibble(round(predict(radius_species, cnh_radius, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")), method = "radius") %>%
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point()+
  geom_line(aes(group = tree_id_nu), alpha = .5)+
  facet_wrap(~tree_type)

cnh_lidar %>%
  bind_cols(as_tibble(round(predict(lidar_species, cnh_lidar, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good"))) %>%
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point()+
  geom_line(aes(group = tree_id_nu), alpha = .5)+
  facet_wrap(~tree_type)

test_data_point %>%
  bind_cols(as_tibble(round(predict(test_point, test_data_point, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good"))) %>%
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point()+
  geom_line(aes(group = tree_id_nu))+
  facet_wrap(~tree_type)

test_data_radius %>%
  bind_cols(as_tibble(round(predict(test_radius, test_data_radius, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good"))) %>%
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point()+
  geom_line(aes(group = tree_id_nu))+
  facet_wrap(~tree_type)

test_data_lidar %>%
  bind_cols(as_tibble(round(predict(test_lidar, test_data_lidar, type = "p"), 3))) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good"))) %>%
  ggplot(aes(x = rating, y = p, color = species))+
  geom_point(alpha = .7)+
  geom_line(aes(group = tree_id_nu))+
  facet_wrap(~tree_type)
```



<!--# Previous work section should say more about the different crown delineation methods and also about using species vs. functional types vs ignoring this information. For example, there is not much discussion of the lidar-based methods and the single pixel method. Also, did Fang et al. (2020) consider species or did they treat all trees the same? Where does this idea that species will improve model performance come from? The level of detail of your review of the methods of the first two main papers in this section is good. Maybe just add another couple papers to flesh out the crown delineation approaches and how incorporating species might improve things.  -->

<!--# Specify that you are replicating the methodology HERE in Portland. You can also mention that previous field work (again you can forward-cite) manually classified trees with a health rating. Then, mention where this thesis takes the work. -->
