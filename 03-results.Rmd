---
bibliography: references.bib
output: pdf_document
nocite: '@*'
---

```{r setup-2, include=FALSE}
knitr::opts_chunk$set(message = F, warning = F, fig.align='center', fig.pos = "H", out.extra = "")
```

```{r, echo = F}
species_pal <- c("#219EBC", "#023047", "#FFB703", "#FB8500", "#586f7c")

health_pal <- c("#83b692", "#f9ada0", "#f9627d", "#586f7c")

method_pal <- c("#FBCF9D", "#7dcfb6", "#f79256")
```

```{r loading-wrangling-data, include = F, message = F, warning = F}
cnh_point <- read_csv("data/cnh_point.csv") %>%
  mutate(health_rat = fct_relevel(health_rat, levels = c("poor", "fair", "good"))) %>%
  filter(!is.na(health_rat)) %>%
  mutate(tree_type = case_when(species == "ACMA" ~ "broadleaf",
                               species == "ACPL" ~ "broadleaf",
                               TRUE ~ "conifer")) %>%
  mutate(z_ndvi = ((sample_ndvi - mean(sample_ndvi))/sd(sample_ndvi)))

cnh_radius <- read_csv("data/cnh_radius.csv") %>%
  mutate(health_rat = fct_relevel(health_rat, levels = c("poor", "fair", "good"))) %>%
  filter(!is.na(health_rat)) %>%
  mutate(tree_type = case_when(species == "ACMA" ~ "broadleaf",
                               species == "ACPL" ~ "broadleaf",
                               TRUE ~ "conifer")) %>%
  mutate(z_ndvi = ((sample_ndvi - mean(sample_ndvi))/sd(sample_ndvi)))

cnh_lidar <- read_csv("data/cnh_lidar.csv") %>%
  mutate(health_rat = fct_relevel(health_rat, levels = c("poor", "fair", "good"))) %>%
  filter(!is.na(health_rat)) %>%
  mutate(tree_type = case_when(species == "ACMA" ~ "broadleaf",
                               species == "ACPL" ~ "broadleaf",
                               TRUE ~ "conifer"))%>%
  mutate(z_ndvi = ((sample_ndvi - mean(sample_ndvi))/sd(sample_ndvi)))

cnh_long <- cnh_point %>%
  bind_rows(cnh_radius) %>%
  bind_rows(cnh_lidar) %>%
  mutate(method = fct_relevel(method, levels = c("point", "radius", "lidar")))
```

# Results {#results}

## Canopy Width and Tree Height Model {#canopy-model-results}

I used a second order polynomial regression to predict both tree height
and crown width from tree DBH. The tree height predictions were used to
filter out trees below 25 feet in height, and the crown width
predictions were used in the radius method of pixel selection for NDVI
analysis. There are numerous expected differences between the species
due to functional tree type. For example, the average DBH for ACMA and
PSME are relatively close, but the average height for a PSME individual
is nearly twice as tall as an ACMA (Table \@ref(tab:tree-stats-table)).
Because of this, DBH would have a very different effect on the outcome
of both models depending on the species of the tree. I included species
as an interaction term in the model to account for these expected
differences between species, meaning that both DBH and species were used
to predict crown width and tree height. I tested a linear model as well
as second and third order polynomial models for both predictive models
(Figures \@ref(fig:test-height-model), \@ref(fig:test-width-model)). A
second order polynomial was chosen for the predictive models because it
not only encapsulated the relationship between DBH and tree height as
well as DBH and canopy width, but it also was the most accurate when
modeling the individual species. When I modeled each species
individually for both tree height and canopy width, I got the same
results as if I created one model and included species as an interaction
term.

```{r, include = F}
# load park data, filter to relevant species, and get average crown width.
my_park <- pdxTrees::get_pdxTrees_parks() %>%
  filter(Species %in% c('ACPL', 'THPL', 'PSME', 'ACMA')) %>%
  mutate(crown_width = (Crown_Width_EW+Crown_Width_NS)/2)
```

```{r tree-stats-table, echo = F, results = 'asis'}
kable(my_park %>% 
  group_by(Species) %>%
  summarise(mean(DBH), mean(Tree_Height), mean(crown_width)),
  booktabs = TRUE, col.names = c("Species", "Avg. DBH", "Avg. Tree Height", "Avg. Crown Width"),
  caption = "Average physiological measurements for selected species in the Park Trees database.",
  caption.short = "Physiological Tree Measurements", digits = 2) %>%
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

The tree height model has an adjusted R-squared value of 0.7834, and a
P-value of \< 2.2e-16 (Figure \@ref(fig:height-model), Appendix
equations \@ref(eq:a1), \@ref(eq:a3), \@ref(eq:a5), \@ref(eq:a7)). This
model was most effective for ACMA and PSME, but was not statistically
significant for ACPL or THPL. This is likely due to the similar average
DBH and tree height values between ACMA and PSME. While the average DBH
values for ACMA and PSME are very similar, the average height values are
extremely different, with the average PSME individual standing twice as
tall as the average ACMA individual (Table \@ref(tab:tree-stats-table)).
An issue with the second degree model is the decrease in predicted tree
height. This decrease is not seen in the park tree model, but the street
tree height predictions for all species show a visible decrease after
the inflection point of the model. Given that the tree height
predictions were only used for filtering out trees below 25 feet in
height, an extremely precise model was not necessary for the purposes of
this thesis.

The crown width model has an adjusted R-squared value of 0.7269, and a
P-value of \< 2.2e-16 (Figure \@ref(fig:width-model-g), Appendix
equations \@ref(eq:a2), \@ref(eq:a4), \@ref(eq:a6), \@ref(eq:a8)). The
coefficients for the canopy width model had statistically significant
p-values for all species. There is consistent variation in the average
crown width measurements between all species (Table
\@ref(tab:tree-stats-table)). With the visualization of the street tree
canopy width predictions, the only extreme decrease after the model
inflection point is seen in the ACPL predictions. The canopy width model
played a larger role in this thesis than the tree height model because
it was essential for the radius method of pixel selection for all street
trees.

```{r tree-model, include = F, cache=TRUE}
# create model for heights
heights <- lm(Tree_Height ~ poly(DBH, degree = 2, raw = T) * Species, data = my_park)
summary(heights)

# create model for crown width
cr_width <- lm(crown_width ~ poly(DBH, degree = 2, raw = T) * Species, data = my_park)
summary(cr_width)

# load street data, filter to relevant species
my_street <- pdxTrees::get_pdxTrees_streets() %>%
  filter(Species %in% c('ACPL', 'THPL', 'PSME', 'ACMA'))

# run street trees through both models
my_street_2 <- my_street %>%
  mutate(width_preds = predict(cr_width, my_street, se.fit = FALSE), 
         height_preds = predict(heights, my_street, se.fit = F))

# export data file
# write_csv(my_street, "data/my_street_2.csv")

# graphing
p_height_model <- my_park %>% 
  ggplot(aes(x = sqrt(DBH), y = Tree_Height, color = Species)) +
  geom_point(alpha = .1)+
  scale_color_manual(values = species_pal)+
  geom_smooth(method = "lm", se = F, formula = y ~ poly(x, degree = 2, raw = T))+
  labs(subtitle = "Measurements from park trees", y = "Tree Height Measurement", title = "Model")+
  guides(color = "none")

p_width_model <- my_park %>% 
  ggplot(aes(x = sqrt(DBH), y = crown_width, color = Species)) +
  geom_point(alpha = .1)+
  scale_color_manual(values = species_pal)+
  geom_smooth(method = "lm", se = F, formula = y ~ poly(x, degree = 2, raw = T))+
  labs(subtitle = "Measurements from park trees", y = "Canopy Width Measurement", title = "Model")+
  guides(color = "none")

s_height_preds <- my_street_2 %>% 
  ggplot(aes(x = sqrt(DBH), y = height_preds, color = Species)) +
  geom_point(alpha = .3)+
  scale_color_manual(values = species_pal)+
  labs(subtitle = "Predictions for street trees", y = "Tree Height Prediction", title = "Predictions")

s_width_preds <- my_street_2 %>% 
  ggplot(aes(x = sqrt(DBH), y = width_preds, color = Species)) +
  geom_point(alpha = .3)+
  scale_color_manual(values = species_pal)+
  labs(subtitle = "Predictions for street trees", y = "Canopy Width Prediction", title = "Predictions")
```

```{r width-model-g, echo = F, fig.scap="Crown width predictive model", out.width = "80%", fig.cap = "Second order polynomial model for predicting tree crown width based on measured DBH and species, with predictions for street trees. (R-squared = 0.7269,  P < 2.2e-16)"}
p_width_model + s_width_preds +
  plot_annotation(title = "Crown Width Model and Predictions")
```

## Health Rating vs. Tree Type and Tree Species

```{r chi-square, include = F}
cnh_long_unique <- cnh_long %>%
  distinct(tree_id_nu, health_rat, species, tree_type)

tree_type_table <- table(cnh_long_unique$health_rat, cnh_long_unique$tree_type)
tree_species_table <- table(cnh_long_unique$health_rat, cnh_long_unique$species)

chisq.test(tree_type_table)
chisq.test(tree_species_table)
```

For all trees collected as part of the CNH field work, I examined the
potential existing relationships between measured health rating and tree
functional type, as well as between health rating and tree species. A
Chi-Square Test of Independence was performed to assess the relationship
between health rating and functional tree type. There was a significant
relationship between the two variables ($X^2$(2, N=112) = 10.184, p =
0.00615). Within functional tree type, both conifers and broadleaf trees
were most likely to be rated as **fair**, but within health category a
**good** or a **poor** tree were most likely a broadleaf tree, and a
**fair** tree was most likely a conifer (Tables
\@ref(tab:table-freq-test-1), \@ref(tab:table-freq-test-2)). A
Chi-Square Test of Independence was performed to assess the relationship
between health rating and tree species. There was a significant
relationship between the two variables ($X^2$(6, N=112) = 22.368, p =
0.00104). Within tree species, ACMA, PSME, and THPL were all most likely
to be rated as **fair**, but ACPL was most likely to be rated as
**good**. Within health category, a **poor** tree was most likely an
ACMA individual, a **fair** tree was equally likely to be a PSME or THPL
individual, and a **good** tree was most likely an ACPL individual
(Tables \@ref(tab:table-freq-test-3), \@ref(tab:table-freq-test-4)).

```{r table-freq-test-1, echo = F, results = 'asis'}
kable(tree_type_table, 
      booktabs = TRUE, longtable = TRUE,
      caption = 'Frequency table of functional tree type and health rating for CNH trees. There is a statistically significant relationship between the two variables ($X^2$(2, N=112) = 10.184, p = 0.00615)', 
      caption.short = 'Frequency table of functional tree type and health rating for CNH trees'
) %>%
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

```{r table-freq-test-2, echo = F, results = 'asis'}
kable(tree_type_table/112, 
      booktabs = TRUE, longtable = TRUE,
      caption = 'Probability table of functional tree type and health rating for CNH trees. ', 
      caption.short = 'Probability table of functional tree type and health rating for CNH trees'
) %>%
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

```{r table-freq-test-3, echo = F, results = 'asis'}
kable(tree_species_table, 
      booktabs = TRUE, longtable = TRUE,
      caption = 'Frequency table of functional tree type and health rating for CNH trees. There is a statistically significant relationship between the two variables ($X^2$(2, N=112) = 10.184, p = 0.00615)', 
      caption.short = 'Frequency table of functional tree type and health rating for CNH trees'
) %>%
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

```{r table-freq-test-4, echo = F, results = 'asis'}
kable(tree_species_table/112, 
      booktabs = TRUE, longtable = TRUE,
      caption = 'Probability table of functional tree type and health rating for CNH trees.', 
      caption.short = 'Probability table of functional tree type and health rating for CNH trees'
) %>%
  kableExtra::kable_styling(latex_options = "HOLD_position")
```

## Health Rating vs. NDVI: Point Method Obtained Data

With the selection of CNH trees and 2021 NDVI data, a statistical
analysis of the point value method shows that there is a statistically
significant difference in NDVI values between health categorizations of
**fair** and **good** (ANOVA, $F_{2, 109} = 3.892$, $P = 0.023$,
TukeyHSD). There is a general positive correlation between NDVI and
health category, specifically **fair** and **good**. The relationship
between health and NDVI is more apparent in the two maple species, but
still holds in the **fair** and **good** categories in the two
coniferous species (Figure \@ref(fig:point-species)). While the
difference between NDVI values and health ratings is statistically
significant for the point data as a whole, this is not true for all
species. The difference in NDVI between **fair** and **good** rated
trees is only statistically significant for THPL, meaning that this
statistically significant relationship in the point method data is
entirely driven by the THPL measurements (ANOVA: $F_{2, 25} = 3.834$,
$P = 0.0353$; TukeyHSD: **good**-**fair** $P = 0.031$). There is no
statistical significance when split by functional tree type.

```{r, include = F}
summary(aov(sample_ndvi ~ health_rat, data = cnh_point))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_point))

summary(aov(sample_ndvi ~ health_rat, data = cnh_point %>% filter(species == "THPL")))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_point %>% filter(species == "THPL")))
```

(ref:my-caption-1) NDVI extracted for CNH trees using the point method
compared to CNH tree health categorization. There is a statistically
significant difference between **fair** and **good** (ANOVA,
$F_{2, 109} = 3.892$, $P = 0.023$, TukeyHSD)

```{r point-species, echo = F, out.width = "100%", fig.scap = "Point Method NDVI and Health Rating, by Species", fig.cap="(ref:my-caption-1)"}
cnh_point %>%
  ggplot(aes(x = health_rat, y = sample_ndvi))+
  scale_color_manual(values = species_pal)+
  geom_boxplot()+
  geom_point(aes(color = species), alpha = .2, size = 3)+
  facet_wrap(~species)+
  labs(x = "Health Rating", y = "NDVI", title = "Point Method NDVI and Health Rating for CNH trees")
```

## Health Rating vs. NDVI: Radius Method Obtained Data

With the radius method of pixel selection and crown delineation, there
is a statistically significant difference in the mean NDVI values for
health categories (ANOVA, $F_{2,109}=4.923$, $P = 0.0089$; TukeyHSD:
**good-poor** $P = 0.033$, **good-fair** $P = 0.014$). Similar to the
point method, there is a larger difference in average NDVI between the
**fair** vs **good** categories than **poor** vs **fair**, which is
statistically significant in the radius method results (Figure
\@ref(fig:radius-species)). When the statistical analysis is split by
tree species, there is no statistical significance. When split by
functional tree type, there is a statistcally significant difference in
the NDVI values between **good** and **poor** health ratings for
broadleaf trees (ANOVA, $F_{2,56}=3.976$, $P = 0.0243$, TukeyHSD).

```{r, include = F}
summary(aov(sample_ndvi ~ health_rat, data = cnh_radius))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_radius))

summary(aov(sample_ndvi ~ health_rat, data = cnh_radius %>% filter(tree_type == "broadleaf")))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_radius %>% filter(tree_type == "broadleaf")))
```

```{r radius-species, echo = F, fig.scap = "Radius method mean NDVI and health condition by species", fig.cap = "Boxplot of average NDVI from radius method and health condition, by species. The relationship between NDVI and health category is strongest in the maples, but still somewhat aparent in the conifers. (ANOVA, $F_{2,109}=4.923$, $P = 0.0089$; TukeyHSD: good-poor $P = 0.033$, good-fair $P = 0.014$)", out.width = "100%"}
cnh_radius %>%
  ggplot(aes(x = health_rat, y = sample_ndvi))+
  scale_color_manual(values = species_pal)+
  geom_boxplot()+
  geom_point(aes(color = species), alpha = .2, size = 3)+
  facet_wrap(~species)+
  labs(x = "Health Rating", y = "Mean NDVI", title = "Radius Method NDVI and Health Rating for CNH Trees")
```

## Health Rating vs. NDVI: LiDAR Method Obtained Data

There is a statistically significant difference in NDVI between health
categories for the LiDAR method, specifically between **good** and
**fair** (ANOVA, $F_{2,101} = 5.405$, $P = 0.00589$, TukeyHSD). The same
relationship between NDVI and health condition for the maples that was
seen in the point and radius method is seen here (Figure
\@ref(fig:lidar-species)). Again, when split by species, there is no
statistical significance. However, when split by functional tree type
there is a statistically significant difference between the average NDVI
values for **good** and **poor** health categories for broadleaf trees
(ANOVA, $F_{2,51} = 3.498$, $P = 0.0377$, TukeyHSD).

```{r, include = F}
summary(aov(sample_ndvi ~ health_rat, data = cnh_lidar))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_lidar))

summary(aov(sample_ndvi ~ health_rat, data = cnh_lidar %>% filter(tree_type == "broadleaf")))
TukeyHSD(aov(sample_ndvi ~ health_rat, data = cnh_lidar %>% filter(tree_type == "broadleaf")))
```

(ref:my-cap-2) Boxplot of average NDVI from LiDAR method and health
condition for each species. Due to the nature of the LiDAR delineation
algorithm, no crown was detected for the PSME individual with a **poor**
health rating. The difference between **good** and **fair** average
NDVIs is statistically significant (ANOVA, $F_{2,101} = 5.405$,
$P = 0.00589$, TukeyHSD)

```{r lidar-species, echo = F, out.width = "100%", fig.scap = "NDVI and health condition for LiDAR method", fig.cap = "(ref:my-cap-2)"}
cnh_lidar %>%
  ggplot(aes(x = health_rat, y = sample_ndvi))+
  scale_color_manual(values = species_pal)+
  geom_boxplot()+
  geom_point(aes(color = species), alpha = .2, size = 3)+
  facet_wrap(~species)+
  labs(x = "Health Rating", y = "Mean NDVI", title = "LiDAR Method NDVI and Health Rating for CNH Trees")
```

Another aspect of the LiDAR delineation method is the potential for data
loss. Five ACMA and three PSME individuals were lost in the LiDAR crown
delineation process. If this was with a very large sample size the data
loss would likely not be detrimental but especially with a sample size
such as mine, a loss of eight trees is a loss of 5% of the total data,
17% loss for ACMA, and 12% loss for THPL.

## Method Comparison

There is no statistically significant difference in NDVI values for the
three different pixel selection methods, which were standardized to
z-scores for NDVI (ANOVA, $F_{2, 325}=0$, $P = 1$). For each method,
Z-scores were calculated for the NDVI values. The mean Z-score for point
method NDVI for trees rated **poor** is above zero meaning that the raw
NDVI value for a **poor** rated tree is higher than the mean average
NDVI value for point method trees. For the radius and LiDAR data, the
average z-score for **poor** rated trees is below the mean average NDVI
value for each method. Interestingly, the mean z-score for **poor**
LiDAR trees is higher than for **fair** trees (Figure
\@ref(fig:methods-all-species)).

When the species are clumped with raw NDVI measurements, the three
methods show very similar patterns, though the NDVI values are higher
for the point method than either of the other two methods (Figure
\@ref(fig:methods-species)). When all three methods are compared along
with species, we can see that the general trend of health rating and
NDVI is consistent within each species. The pattern between health
rating and NDVI is also very similar between trees of the same
functional type (Figure \@ref(fig:methods-species)). Because of these
similarities in health rating and NDVI values for trees belonging to the
same functional type, I included additional testing examining the
difference in species specificity addition versus functional type
separation.

```{r, include = F}
summary(aov(z_ndvi ~ method, data = cnh_long))
```

```{r methods-all-species, echo = F, out.width = "80%", fig.scap = "Z-score of NDVI and health rating comparison across methods", fig.cap = "Z-score of NDVI values from all three pixel selection methods with CNH health rating. There is no statistically significant difference in NDVI trends between pixel selection methods. (ANOVA, $F_{2, 325}=0$, $P = 1$)"}
cnh_long %>%
  ggplot(aes(y = z_ndvi, x = health_rat, color = method))+
  scale_color_manual(values = method_pal)+
  geom_boxplot()+
  facet_wrap(~method)+
  labs(x = "Health categorization", y = "Sample NDVI")+
  guides(color = "none")
```

```{r methods-species, echo = F, out.width = "90%", fig.scap = "Average NDVI comparison between species and methods.", fig.cap = "Average NDVI for each health category, split by functional tree type and pixel selection method. The general trends remain the same for each individual species, but varies between functional type. The same trends can be seen for trees of the same functional type (broadleaf vs conifer)."}
cnh_long %>%
  ggplot(aes(y = sample_ndvi, x = health_rat, color = species))+
  scale_color_manual(values =species_pal)+
  geom_boxplot()+
  facet_grid(rows = vars(tree_type), cols = vars(method))+
  labs(x = "Health categorization", y = "Sample NDVI")
```

Another important factor to consider when comparing the three pixel
selection methods is the amount of pixels used in the NDVI analysis. For
the point method, each NDVI value is based off one singular point. For
both LiDAR and Radius methods, the NDVI used in prediction is an average
of all NDVI values captured within the pixel selection method, but the
number of pixels used varies between methods and species (Figure
\@ref(fig:ndvi-counts-graph)).

```{r ndvi-counts-code, include = F}
radius_sample_data <- read_csv("data/radius_sample_data.csv") %>%
  dplyr::select(-4, -5) %>% 
  mutate(method = "radius")

lidar_sample_data <- read_csv("data/lidar_sample_data.csv") %>%
  dplyr::select(-4) %>%
  mutate(method = "LiDAR")

count_sample_data <- lidar_sample_data %>%
  bind_rows(radius_sample_data)

ndvi_count_box <- ggplot(data = count_sample_data, aes(x = method, y=ndvi_count, color = species))+
  geom_boxplot(outlier.shape=NA)+
  geom_point(position=position_jitterdodge(jitter.width = .1), alpha = .3)+
  scale_color_manual(values = species_pal)+
  labs(x = "Pixel selection method", y = "Pixel count", color = "Tree species")+
  guides(color = "none")

ndvi_count_point <- ggplot(data = count_sample_data, aes(x = ndvi_mean, y=ndvi_count, color = species))+
  geom_point()+
  scale_color_manual(values = species_pal)+
  labs(x = "Average NDVI", y = "Pixel count", color = "Tree species")
```

```{r ndvi-counts-graph, echo = F, fig.scap = "Pixel counts for NDVI calculation, by method, and for sample NDVI.", fig.cap = "Comparison of pixel counts used in NDVI calculations. (a) Boxplot of pixel counts used for each species in the LiDAR and radius methods. For both methods, the maples had more outliers than the conifers, and THPL had the lowest average number of pixels used. (b) Pixel count and average NDVI per tree.", out.width = "90%"}
ndvi_count_box + ndvi_count_point + 
  plot_annotation(tag_levels = "a", tag_suffix = ")")
```

## Predictive Model

```{r point-modeling, include = F, cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
point_all <- polr(health_rat~sample_ndvi, Hess = TRUE, data = cnh_point)
brant::brant(point_all)

point_all_preds <- predict(point_all, cnh_point)
cfm_p_all <- confusionMatrix(point_all_preds, cnh_point$health_rat)

p_all <- ggplotConfusionMatrix(cfm_p_all)

point_type <- polr(health_rat~sample_ndvi*tree_type, Hess = TRUE, data = cnh_point)
brant::brant(point_type)

point_type_preds <- predict(point_type, cnh_point)
cfm_p_type <- confusionMatrix(point_type_preds, cnh_point$health_rat)

p_type <- ggplotConfusionMatrix(cfm_p_type) 

point_species<- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = cnh_point)
brant::brant(point_species)

point_species_preds <- predict(point_species, cnh_point)
cfm_p_species <- confusionMatrix(point_species_preds, cnh_point$health_rat)

p_species <- ggplotConfusionMatrix(cfm_p_species)
```

```{r p-model-summaries, include = F}
summary(point_all)
cfm_p_all$overall

summary(point_type)
cfm_p_type$overall

summary(point_species)
cfm_p_species$overall

probs_point_species <- as_tibble(round(predict(point_species, cnh_point, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))

point_probs <- ggplot(probs_point_species, aes(x = rating, y = p))+
  geom_boxplot()
```

I used ordinal logistic regression models with the goal of predicting
tree health rating from NDVI. In order to determine the impact of
differentiating by species in predictive models, I ran three different
models for each point selection method, moving from least to most
specific: a model that only uses NDVI to predict health rating, a model
that includes functional tree type as an interaction term, and a third
model that includes species as an interaction term (Table
\@ref(tab:model-table)).

Due to the limited sample size of the data I am working with, the
following models are trained and tested on the same data, which is the
CNH tree data collected summer 2021. I chose to do this because of the
limited sample size of CNH trees, and the uneven size of the health
category ratings. Because of this, it is important to note that the
model tests are an idealized version of a true model. However, it is
important to note that they are idealized version of how model testing
and training would work in reality.

With the point method data, the first model using only NDVI as a
predictor (health rating \~ point NDVI) led to all tree points being
rated as **fair** (Figure \@ref(fig:point-model-split)a). For the
continuous variable of NDVI, the coefficient value of 7.041 can be
interpreted as that with one unit increase in NDVI, the log of odds of
having a **good** health rating increase by 7.041. With the t-value of
2.479, this is statistically significant at the .05 level. The intercept
for **poor** vs **fair** shows that the log of odds of a tree being
rated as **poor** versus **fair** or **good** is not statistically
significant, but the log odds of a tree being rated **poor** or **fair**
versus **good** are statistically significant.

The model using tree type as an additional predictor (health rating \~
point NDVI \* tree type) had an increase in predictions of **good**
health trees, and increased kappa from 0% to 15% (Figure
\@ref(fig:point-model-split)b). The overall accuracy only increased by
2%, but kappa is more informative in terms of model validity than the
accuracy score. With this model, one unit increase in NDVI will lead to
an increase in the log odds of having a **good** rating by 10.2 (t-value
= 2.685). The impact of a tree being classified as a conifer, versus a
broadleaf tree, were not statistically significant at the 0.05 level.
Again, the intercept for **poor** vs **fair** is not statistically
significant, but the log odds of a tree being rated **poor** or **fair**
versus **good** are statistically significant.

Using tree species instead of tree type in the model (health rating \~
point NDVI \* species) further improved the kappa of the model. The
number of **good** trees that were correctly predicted increased from 7
to 13, but additionally the number of **fair** trees that were correctly
predicted dropped from 64 to 59, with the new predictions as **good**
(Figure \@ref(fig:point-model-split)c). None of the models that used the
point method obtained data were able to correctly predict any trees with
**poor** health categorization, and in models b and c, each had one
**poor** tree that was incorrectly predicted as **good**.

```{r point-model-split, echo = F, out.width = "90%", fig.scap = "Confusion matrixes for Point method predictive models", fig.cap = "Confusion Matrix results for three predictive models, using point method obtained NDVI data to predict three health rating. (a) Model only using NDVI to predict health rating. (b) Model using both NDVI and functional tree type (broadleaf vs confier) to predict health rating. (c) Model using NDVI and tree species to predict health rating."}
p_all + 
  labs(subtitle = "health rating ~ point NDVI") +
  p_type + 
  labs(subtitle = "health rating ~ point NDVI * tree type") +
  p_species + 
  labs(subtitle = "health rating ~ point NDVI * species") +
  plot_annotation(tag_levels = "a", tag_suffix = ")", title = "Confusion Matrixes for Point Method NDVI Models") + 
  plot_layout(ncol = 2)
```

```{r radius-modeling, include = F, cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
radius_all <- polr(health_rat~sample_ndvi, Hess = TRUE, data = cnh_radius)
radius_all_preds <- predict(radius_all, cnh_radius)
cfm_r_all <- confusionMatrix(radius_all_preds, cnh_radius$health_rat)
r_all <- ggplotConfusionMatrix(cfm_r_all)

radius_type <- polr(health_rat~sample_ndvi*tree_type, Hess = TRUE, data = cnh_radius)
radius_type_preds <- predict(radius_type, cnh_radius)
cfm_r_type <- confusionMatrix(radius_type_preds, cnh_radius$health_rat)

r_type <- ggplotConfusionMatrix(cfm_r_type)

radius_species <- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = cnh_radius)
radius_species_preds <- predict(radius_species, cnh_radius)
cfm_r_species <- confusionMatrix(radius_species_preds, cnh_radius$health_rat)

r_species <- ggplotConfusionMatrix(cfm_r_species)
```

```{r r-model-summaries, include = F}
summary(radius_all)
cfm_r_all$overall

summary(radius_type)
cfm_r_type$overall

summary(radius_species)
cfm_r_species$overall

probs_radius_species <- as_tibble(round(predict(radius_species, cnh_radius, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))
```

Using just NDVI as a predictor in the model (health rating \~ radius
NDVI) still performs the worst of the three models, with 96% of the
sampled trees being predicted as **fair** (Figure
\@ref(fig:radius-model-split)a). Similar to the point method with no
interaction term (health rating \~ point NDVI), the **fair** vs **good**
intercept is the only one that is statistically significant. However,
this model has a t value of 3.68, meaning this model performs better
than the point method model with the same formula. When using tree type
as predictor variables (health rating \~ radius NDVI \* tree type),
there was an increase in the number of trees being rated as in **good**
health of 9 trees in total, but only 6 of those were additional correct
predictions (Figure \@ref(fig:radius-model-split)b). Again, the
intercept of **fair** vs **good** is statistically significant, meaning
that the log of odds of a tree in this model being rated **poor** or
**fair** versus **good**. The main takeaway from the radius method
models is that with the inclusion of species as an interaction term with
the radius data (health rating \~ radius NDVI \* species), we see our
first predictions of the **poor** category. There is no increase in
statistical significance of the log odds between the previous model and
this one. The radius method does involve more pixels than the point
method, and with the range in NDVI values being lower than that of the
point method it makes sense that there is an increase in **poor** rated
trees. However, out of the 15 reference trees with health ratings of
**poor**, only one was correctly predicted as such (Figure
\@ref(fig:radius-model-split)c).

```{r radius-model-split, echo = F, out.width = "90%", fig.scap = "Confusion matrixes for Radius method predictive models", fig.cap = "Confusion matrixes for each species predictions with radius method data. (a) Model using only NDVI to predict health rating. (b) Model with the addition of functional tree type as a predictor. (c) Model using tree species as a variable when predicting tree health rating."}
r_all + 
  labs(subtitle = "health rating ~ radius NDVI")+ 
  r_type +
  labs(subtitle = "health rating ~ radius NDVI * tree type") +
  r_species + 
  labs(subtitle = "health rating ~ radius NDVI * species") +
  plot_annotation(tag_levels = "a", tag_suffix = ")", 
                  title = "Confusion Matrixes for Radius Method NDVI Models") + 
  plot_layout(ncol = 2)
```

```{r lidar-modeling, include = F, cache=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
lidar_all <- polr(health_rat~sample_ndvi, Hess = TRUE, data = cnh_lidar)
lidar_all_preds <- predict(lidar_all, cnh_lidar)
cfm_l_all <- confusionMatrix(lidar_all_preds, cnh_lidar$health_rat)

l_all <- ggplotConfusionMatrix(cfm_l_all)

lidar_type <- polr(health_rat~sample_ndvi*tree_type, Hess = TRUE, data = cnh_lidar)
lidar_type_preds <- predict(lidar_type, cnh_lidar)
cfm_l_type <- confusionMatrix(lidar_type_preds, cnh_lidar$health_rat)

l_type <- ggplotConfusionMatrix(cfm_l_type)

lidar_species <- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = cnh_lidar)
lidar_species_preds <- predict(lidar_species, cnh_lidar)
cfm_l_species <- confusionMatrix(lidar_species_preds, cnh_lidar$health_rat)

l_species <- ggplotConfusionMatrix(cfm_l_species)
```

```{r l-model-summaries, include = F}
summary(lidar_all)
cfm_l_all$overall

summary(lidar_type)
cfm_l_type$overall

summary(lidar_species)
cfm_l_species$overall

probs_lidar_species <- as_tibble(round(predict(lidar_species, cnh_lidar, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))
```

The LiDAR data was the most effective at predicting tree health across
all three pixel selection methods. With the first model using only NDVI
to predict health rating (health rating \~ LiDAR NDVI), more trees were
correctly predicted as **good** than the same model with the other two
methods (Figure \@ref(fig:lidar-model-split)a). With the addition of
tree type as a predictor variable (health rating \~ LiDAR NDVI \* tree
type), the number of correctly predicted **good** trees increased, but
most importantly, the number of **fair** trees predicted as **good**
remained the same where with the other two methods, that count had
increased as well. However, this model also included a **good** tree
being predicted as **poor** which no other variations of the models did
(Figure \@ref(fig:lidar-model-split)b). The last model with species as
an interaction term (health rating \~ LiDAR NDVI \* species) was the
most effective of all 9 models, with the highest accuracy and kappa, as
well as the smallest p-value (Figure \@ref(fig:lidar-model-split)c).

```{r lidar-model-split, echo = F, out.width = "90%", fig.scap = "Confusion matrixes for LiDAR method predictive models", fig.cap = "Confusion matrixes for each species predictions with LiDAR method obtained NDVI values. (a) Model using NDVI to predict tree health category. (b) Model using functional tree type in addition to NDVI for health prediction. (c) Model with species as a predictor variable for NDVI based tree health category."}
(l_all +
  labs(subtitle = "health rating ~ LiDAR NDVI")+
  l_type + 
  labs(subtitle = "health rating ~ LiDAR NDVI * tree type")+
  l_species) + 
  labs(subtitle = "health rating ~ LiDAR NDVI * species")+
  plot_annotation(tag_levels = "a", tag_suffix = ")", title = "Confusion Matrixes for LiDAR Method NDVI Models") + 
  plot_layout(ncol = 2)
```

```{r, include = F, tidy=TRUE, tidy.opts=list(width.cutoff=60)}
probability_data <- probs_point_species %>%
  mutate(method = "health ~ point NDVI * species") %>%
  bind_rows(probs_radius_species %>%
  mutate(method = "health ~ radius NDVI * species")) %>%
  bind_rows(probs_lidar_species %>%
  mutate(method = "health ~ LiDAR NDVI * species")) %>%
  mutate(method = fct_relevel(method, levels = c("point", "radius", "lidar")))

prob_graph_1 <- ggplot(probability_data, aes(x = rating, y = p))+
  geom_boxplot() +
  facet_wrap(~method) + 
  labs(title = "Probability of Health Rating Categorization")
```

```{r prob-graph-1, fig.cap = "Boxplot of probabilities of health group categorization", echo = F}
prob_graph_1
```

With the results of the models using species as an interaction term for
each of the three pixel selection methods (health rating \~ point NDVI
\* species; health rating \~ radius NDVI \* species; health rating \~
LiDAR NDVI \* species), I compared the probabilities of a given tree
being assigned each health category (Figure \@ref(fig:prob-graph-1)).
For each prediction, a probability of assignment to each health category
is created (Table \@ref(tab:preds-head)). The probability of a tree
being predicted as **good** stays fairly consistent across the three
methods, but the most interesting aspect is the probability of a
**poor** rating, which increases across models. This is consistent with
the confusion matrixes created for each model.

Given the uneven number of data points available for the varying health
conditions, I down-sampled each health rating to the smallest number of
points in a given category (Point = 15, Radius = 15, LiDAR = 13). When
the predictive models were ran with equal category sizes, the results
were statistically significant for all three pixel selection models. The
point and radius method models correctly predicted the health ratings
for most of the **poor** and **good** rated trees but not for **fair**
rated trees, which is the opposite of what we saw with the previous
models. The LiDAR model had the highest proportion of correctly
predicted health ratings across all three health categories (Figure
\@ref(fig:final-model-small)).

```{r downsample-test, include = F, cache=TRUE}
set.seed(2)
test_data_point <- cnh_long %>%
  filter(method == "point") %>%
  group_by(health_rat) %>%
  sample_n(15)

test_point <- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = test_data_point) 
preds_point <- predict(test_point, test_data_point)
cfm_point <- confusionMatrix(preds_point, test_data_point$health_rat)

probs_point_small <- as_tibble(round(predict(test_point, test_data_point, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))


gg_point <- ggplotConfusionMatrix(cfm_point)  + 
  labs(subtitle = "Point method")

set.seed(7)
test_data_radius <- cnh_long %>%
  filter(method == "radius") %>%
  group_by(health_rat) %>%
  sample_n(15)

test_radius <- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = test_data_radius) 

probs_radius_small <- as_tibble(round(predict(test_radius, test_data_radius, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))

preds_radius <- predict(test_radius, test_data_radius)
cfm_radius <- confusionMatrix(preds_radius, test_data_radius$health_rat)

gg_radius <- ggplotConfusionMatrix(cfm_radius)  + 
  labs(subtitle = "Radius method")

set.seed(9)
test_data_lidar <- cnh_long %>%
  filter(method == "lidar") %>%
  group_by(health_rat) %>%
  sample_n(13)

test_lidar <- polr(health_rat~sample_ndvi*species, Hess = TRUE, data = test_data_lidar) 

probs_lidar_small <- as_tibble(round(predict(test_lidar, test_data_lidar, type = "p"), 3)) %>%
  pivot_longer(cols = c("good", "fair", "poor"), names_to = "rating", values_to = "p") %>%
  mutate(rating = fct_relevel(rating, levels = c("poor", "fair", "good")))

preds_lidar <- predict(test_lidar, test_data_lidar)
cfm_lidar <- confusionMatrix(preds_lidar, test_data_lidar$health_rat)

gg_lidar <- ggplotConfusionMatrix(cfm_lidar)  + 
  labs(subtitle = "LiDAR method")
```

```{r, include = F}
probability_data_2 <- probs_point_small %>%
  mutate(method = "health ~ point NDVI * species") %>%
  bind_rows(probs_radius_small %>%
  mutate(method = "health ~ radius NDVI * species")) %>%
  bind_rows(probs_lidar_small %>%
  mutate(method = "health ~ LiDAR NDVI * species")) %>%
  mutate(method = fct_relevel(method, levels = c("point", "radius", "lidar")))

prob_graph_2 <- ggplot(probability_data_2, aes(x = rating, y = p))+
  geom_boxplot() +
  facet_wrap(~method) + 
  labs(title = "Probability of Health Rating Categorization for downsampled models", subtitle = "health rating ~ NDVI * Species")
```

I repeated the probability of assignment analysis with the downsampled
models (Figure \@ref(fig:downsample-probs)). The probabilities of
assignment to each health category are much more even now, though the
average probability for a **fair** prediction is still higher than the
average probability of a **poor** or **good** rating prediction.

```{r downsampled-models-summary, include = F}
summary(test_point)
cfm_point$overall

tidy(test_radius)
cfm_radius$overall

summary(test_lidar)
cfm_lidar$overall
```

```{r final-model-small, echo = F, out.width = "90%", fig.cap = "Confusion matrixes for downsampled predictive models"}
gg_point + gg_radius + gg_lidar +
  plot_layout(ncol = 2)
```

```{r downsample-probs, echo = F, fig.cap = "Probability of Assignment Analysis with Downsampled Data"}
prob_graph_2
```

```{r final-lidar-model, include = F, cache = T}
lidar_final_data <- lidar_sample_data %>%
  rename(sample_ndvi = ndvi_mean)

final_lidar_preds <- predict(test_lidar, lidar_final_data)

lidar_final_data_2 <- lidar_final_data %>%
  mutate(preds = final_lidar_preds) %>%
  mutate(condition = case_when(condition == "Good" ~ "good",
                               condition == "Fair" ~ "fair",
                               condition == "Poor" ~ "poor")) %>%
  mutate(condition = fct_relevel(condition, levels = c("poor", "fair", "good"))) %>%
  mutate(preds = fct_relevel(preds, levels = c("poor", "fair", "good")))


final_cfm <- confusionMatrix(final_lidar_preds, lidar_final_data_2$condition)
final_plot <- ggplotConfusionMatrix(final_cfm) 

lidar_final_data_2 %>%
  count(species, condition) 

lidar_final_data_2 %>%
  count(species, preds) 
```

<!-- Page 44: Also, did you only run the model for random sample of additional trees once or is this a consensus result from multiple model runs? What about showing a map of your modeled outputs? Maybe a two-panel figure with points colored by species and a second with points colored by health category? -->

To ensure that the results of the downsampled LiDAR data model were not
caused by the random downsampling, I ran the model 6 times with
different random samples each time (Appendix
\@ref(fig:lidar-model-test)). While the values of each predicted
category did differ, the p-value for the model was significant for each
test. The accuracy values ranged from 54% to 67%, and the kappa values
ranged from 31% to 50% (Table \@ref(tab:model-test-table)). 

## Health Predictions for Portland Trees

I used the
final downsampled LiDAR model to predict health rating for a random
sample of 100 park and 100 street trees, with LiDAR calculated NDVI
values. Due to the data loss in the LiDAR processing, the final data set
contains 175 trees (85 street trees, 90 park trees) (Table
\@ref(tab:final-lidar-counts)). This data was input into the final LiDAR
model, created with the downsampled data. A health rating was predicted
based on NDVI and tree species. The resulting health predictions were
graphed along with sample NDVI (Figure \@ref(fig:final-lidar-plot)).
For ACMA, there is a clear relationship between sample NDVI and health
prediction which matches that seen in earlier data analysis. For ACPL,
there is a large difference between the NDVI of **fair** and **good**
trees, which also appears for PSME. for the THPL predictions, the
average NDVI for **poor** trees is higher than those for **fair** trees.

```{r final-lidar-plot, echo = F, out.width = "90%", fig.cap = "Health rating predictions for random sample of Portland trees", fig.align='center'}
lidar_final_data_2 %>%
  ggplot(aes(x = preds, y = sample_ndvi, color = species))+
  geom_boxplot(outlier.shape=NA)+
  geom_point(position=position_jitterdodge(jitter.width = .5), alpha = .3)+
  scale_color_manual(values = species_pal)+
  facet_wrap(~species)+
  labs(x = "Predicted health rating", y = "Average NDVI", title = "LiDAR Model Results")
```
